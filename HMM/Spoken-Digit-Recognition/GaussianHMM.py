from scipy import stats as st
import numpy as np

class GHMM:
    def __init__(self,n_states) -> None:
        self.ns = n_states

        self.A = self._stochastize(np.random.RandomState(0).rand(self.ns,self.ns)) #np.random.randint(0,100)).rand(self.n_states,self.n_states))
        self.pi = self._normalize(np.random.RandomState(0).rand(self.ns,1))

        self.mu = None
        self.cov = None
        self.nd = None
    
    def _normalize(self,X):
        X = (X + (X==0))/np.sum(X)
        return X

    def _stochastize(self,Y):
        Y = (Y+(Y==0))/np.sum(Y,axis=1)
        return Y

    def _initialize_state_gaussian(self,obs):
        '''
            # Initializes mean vector and covariance matrix for the states.
            Each state is inialized to be sort of random gaussian, based on the first training example. 
            Each state_gaussian is initialized to have same covariance which is in fact the covariance of first training example
        '''
        if(self.nd == None):
            self.nd = obs.shape[0]
        if(self.mu == None):
            subset = np.random.RandomState(1).choice(np.arange(self.nd),self.ns,replace=False)
            self.mu = obs[:,subset]
        if(self.cov == None):
            self.cov = np.zeros((self.nd,self.nd,self.ns))
            self.cov += np.cov(obs)

    def _calculate_emission_probability(self,obs):
        '''
        # Calculates Emission Probability; B matrix.
        i.e The probability of ith observation being generated by jth state
        '''
        obs = np.atleast_2d(obs)
        self.B = np.zeros((self.ns,obs.shape[1]))
        for s in range(self.ns):
            self.B[s,:] = st.multivariate_normal.pdf(obs.T,mean=self.mu[s],cov=self.cov[s])

    def _forward(self):
        '''
        # Forward Algorithm
        first part of forward-backward Algorithm
        '''
        self.alpha = np.zeros(self.B.shape)
        log_likelihood = 0.0
        T = self.B.shape[1]
        for t in range(T):
            if t == 0:
                self.alpha[:,t] = self.B[:,t] * self.pi.ravel()
            else:
                self.alpha[:,t] = self.B[:,t] * np.dot(self.A.T,self.alpha[:,t-1])
        
            alpha_sum = np.sum(self.alpha[:,t])
            self.alpha[:,t] /= alpha_sum
            log_likelihood = log_likelihood + np.log(alpha_sum)
        return log_likelihood

    def _backward(self):
        '''
        # Backward Algorithm
        second part of forward-backward algorithm
        '''
        self.beta = np.ones(self.B.shape)
        T = self.B.shape[1]
        for t in range(T-1)[::-1]:
            self.beta[:,t] = np.dot(self.A, self.B[:,t+1]*self.beta[:,t+1])
            self.beta[:,t] /= np.sum(self.beta[:,t])

    def _Baum_Welch_Algorithm(self):
        '''
        # Baum-Welch Algorithm
        This is EM algorithm that uses forward-backward algorithm.
        '''
        pass

    def train(self,obs):
        self._initialize_state_gaussian(obs)
        self._calculate_emission_probability(obs)
        self._forward()
        self._backward()
        self._Baum_Welch_Algorithm(obs)

if __name__ == '__main__':
    print('hello')
    print(np.random.RandomState(0).choice(np.arange(300), size=2, replace=False))
    a = GHMM(2)
    T = 10
    for t in range(T - 1)[::-1]:
        print(t)
