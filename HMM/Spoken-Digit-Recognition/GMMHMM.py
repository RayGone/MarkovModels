from scipy import stats as st
import numpy as np
from numpy import linalg as la


##copied from stackoverflow: https://stackoverflow.com/questions/43238173/python-convert-matrix-to-positive-semi-definite/43244194?fbclid=IwAR16qFZel8kjzgkJHAckWLIgrAgyc7MJz3GgcdnXbCF08RE9TR70Y_lQ1xk
def nearestPD(A):
    """Find the nearest positive-definite matrix to input

    A Python/Numpy port of John D'Errico's `nearestSPD` MATLAB code [1], which
    credits [2].

    [1] https://www.mathworks.com/matlabcentral/fileexchange/42885-nearestspd

    [2] N.J. Higham, "Computing a nearest symmetric positive semidefinite
    matrix" (1988): https://doi.org/10.1016/0024-3795(88)90223-6
    """
    B = (A + A.T) / 2
    _, s, V = la.svd(B)

    H = np.dot(V.T, np.dot(np.diag(s), V))

    A2 = (B + H) / 2

    A3 = (A2 + A2.T) / 2

    if isPD(A3):
        return A3

    spacing = np.spacing(la.norm(A))
    # The above is different from [1]. It appears that MATLAB's `chol` Cholesky
    # decomposition will accept matrixes with exactly 0-eigenvalue, whereas
    # Numpy's will not. So where [1] uses `eps(mineig)` (where `eps` is Matlab
    # for `np.spacing`), we use the above definition. CAVEAT: our `spacing`
    # will be much larger than [1]'s `eps(mineig)`, since `mineig` is usually on
    # the order of 1e-16, and `eps(1e-16)` is on the order of 1e-34, whereas
    # `spacing` will, for Gaussian random matrixes of small dimension, be on
    # othe order of 1e-16. In practice, both ways converge, as the unit test
    # below suggests.
    I = np.eye(A.shape[0])
    k = 1
    while not isPD(A3):
        mineig = np.min(np.real(la.eigvals(A3)))
        A3 += I * (-mineig * k**2 + spacing)
        k += 1

    return A3


def isPD(B):
    """Returns true when input is positive-definite, via Cholesky"""
    try:
        _ = la.cholesky(B)
        return True
    except la.LinAlgError:
        return False


class GMMHMM:
    def __init__(self,n_states) -> None:
        self.ns = n_states # number of hidden states

        self.A = self._stochastize(np.random.RandomState(0).rand(self.ns,self.ns)) #np.random.randint(0,100)).rand(self.n_states,self.n_states))
        self.pi = self._normalize(np.random.RandomState(0).rand(self.ns,1))

        self.mu = None # mean vectors - of states
        self.cov = None # covariance matrices - of states
        self.nd = None # dimension of input - should be equal to number of hidden states i.e. self.ns
    
    def _normalize(self,X):
        X = (X + (X==0))/np.sum(X)
        return X

    def _stochastize(self,Y):
        Y = (Y+(Y==0))/np.sum(Y,axis=1)
        return Y

    def _initialize_state_gaussian(self,obs):
        '''
            # Initializes mean vector and covariance matrix for the states.
            Each state is inialized to be sort of random gaussian, based on the first training example. 
            Each state_gaussian is initialized to have same covariance which is in fact the covariance of first training example
        '''
        if(self.nd == None):
            self.nd = obs.shape[0]
        if(self.mu == None):
            subset = np.random.RandomState(1).choice(np.arange(self.nd),self.ns,replace=False)
            self.mu = obs[:,subset]
        if(self.cov == None):
            self.cov = np.zeros((self.ns,self.nd,self.nd))
            self.cov += np.cov(obs)
            # print(self.cov)

    def _calculate_emission_probability(self,obs):
        '''
        # Calculates Emission Probability; B matrix.
        i.e The probability of ith observation being generated by jth state
        '''
        obs = np.atleast_2d(obs)
        self.B = np.zeros((self.ns,obs.shape[1]))
        for s in range(self.ns):
            # print(self.cov[s],self.mu[s])
            self.B[s,:] = st.multivariate_normal.pdf(obs.T,mean=self.mu[s],cov=self.cov[s],allow_singular=True)

    def _forward(self):
        '''
        # Forward Algorithm
        first part of forward-backward Algorithm
        '''
        self.alpha = np.zeros(self.B.shape)
        log_likelihood = 0.0
        T = self.B.shape[1]
        for t in range(T):
            if t == 0:
                self.alpha[:,t] = self.B[:,t] * self.pi.ravel()
            else:
                self.alpha[:,t] = self.B[:,t] * np.dot(self.A.T,self.alpha[:,t-1])
        
            alpha_sum = np.sum(self.alpha[:,t])
            self.alpha[:,t] /= alpha_sum
            log_likelihood = log_likelihood + np.log(alpha_sum)
        return log_likelihood

    def _backward(self):
        '''
        # Backward Algorithm
        second part of forward-backward algorithm
        '''
        self.beta = np.ones(self.B.shape)
        T = self.B.shape[1]        
        self.beta[:, -1] = np.ones(self.B.shape[0])

        for t in range(T-1)[::-1]:
            self.beta[:,t] = np.dot(self.A, self.B[:,t+1]*self.beta[:,t+1])
            self.beta[:,t] /= np.sum(self.beta[:,t])

    def _Baum_Welch_Algorithm(self,obs):
        '''
        # Baum-Welch Algorithm
        This is EM algorithm that uses forward-backward algorithm.
        '''
        T = obs.shape[1]
        ksi = np.zeros((self.ns,self.ns))
        gamma = np.zeros((self.ns,T))

        for t in range(T-1):
            tmp_g = self.alpha[:,t] * self.beta[:,t]
            gamma[:,t] = self._normalize(tmp_g)

            tmp_k = self.A * np.dot(self.alpha[:,t],self.beta[:,t+1] * self.B[:,t+1].T)
            ksi += self._normalize(tmp_k)
         
        tmp_g = self.alpha[:,-1] * self.beta[:,-1]
        gamma[:,-1] = self._normalize(tmp_g)

        ksi = self._stochastize(ksi) 
        self.A = ksi # -ksi- is the expected state transition matrix
        for i in range(gamma.shape[0]):
            gamma[i] = self._normalize(gamma[i])
            gamma[i] /= np.min(gamma[i])

        #maximizing the states. i.e. mean vector and covariance
        expected_mu = np.zeros((self.nd,self.ns))
        expected_cov = np.zeros((self.nd,self.nd,self.ns))

        gamma_state_sum = np.sum(gamma,axis=1)
        gamma_state_sum += (gamma_state_sum==0)

        self.pi = self._normalize(gamma_state_sum/T) # expected prior distribution of state

        for s in range(self.ns):
            gamma_obs = obs * gamma[s]
            expected_mu[s] = np.sum(gamma_obs, axis=1) / gamma_state_sum[s]

            partial_covs = gamma_obs.T - expected_mu[s]
            partial_covs = np.dot(partial_covs.T,partial_covs) / gamma_state_sum
            partial_covs = np.triu(partial_covs) + np.triu(partial_covs).T - np.diag(np.diag(partial_covs))
            expected_cov[s] = nearestPD(partial_covs)            

        #Ensure positive semidefinite by adding diagonal loading
        expected_cov += .01 * np.eye(self.nd)[:, :, None]

        self.mu = expected_mu
        self.cov = expected_cov


    def train(self,obs,n_iter):
        flag = 2
        if(len(obs.shape) == 3):
            flag = 3
            self._initialize_state_gaussian(obs[0])
            for i in range(n_iter):
                print('training round: ',i)
                for o in obs:                
                    self._calculate_emission_probability(o)
                    self._forward()
                    self._backward()
                    self._Baum_Welch_Algorithm(o)

        elif(len(obs.shape) == 2):
            self._initialize_state_gaussian(obs)     
            for i in range(n_iter):
                print('training: ',i)
                self._calculate_emission_probability(obs)
                self._forward()
                self._backward()
                self._Baum_Welch_Algorithm(obs)
        else:
            raise Exception('Training Data Dimension Error; Expected 2d (num_states,*) or 3d (*,num_states,*)')

    def score(self,obs):        
        self._calculate_emission_probability(obs)
        log_likelihood = self._forward()
        return log_likelihood